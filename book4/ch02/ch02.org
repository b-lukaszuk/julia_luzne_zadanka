#+TITLE: Ch02. Bayesâ€™s Theorem
#+STARTUP: overview
#+STARTUP: indent
#+OPTIONS: \n: t

* Link to the chapter online
[[https://allendowney.github.io/ThinkBayes2/chap02.html][Chapter 2]]

* Warning
*The content of this file may be incorrect, erroneous and/or harmful. Use it at Your own risk.*

* Data Files
TODO
* Environment

#+BEGIN_SRC

println("Let's go.")

#+END_SRC

#+BEGIN_SRC

cd(joinpath(homedir(), "Desktop", "julia/luzne_zadanka/book4/ch02")

#+END_SRC

#+BEGIN_SRC

using Pkg
Pkg.activate(".")
Pkg.status()

#+END_SRC

* Imports

#+BEGIN_SRC

import DataFrames as Dfs

#+END_SRC

* Bayes's Theorem

$P(A|B) = \frac{P(A) * P(B|A)}{P(B)}$

* 2.1. The Cookie Problem

There are 2 bowls of cookies:

- Bowl 1 contains 30 vanilla cookies and 10 chocolate cookies.
- Bowl 2 contains 20 vanilla cookies and 20 chocolate cookies.

You choose a bowl at random and choose a cookie from it. The cookie is vanilla.
What is the probability that it came from Bowl1?

My (BL) solution (I solved it alone):

30v+20v = 50v

P(B1) = 30/50 = 3/5 = 6/10 = 0.6

P(B2) = 20/50 = 2/5 = 2/10 = 0.4

or from Beyes's Theorem

$P(A|B) = \frac{P(A) * P(B|A)}{P(B)}$

after replacing A with B1 and B with V, we get:

$P(B1|V) = \frac{P(B1) * P(V|B1)}{P(V)}$

and

$P(B1|V) = \frac{\frac{1}{2} * \frac{30}{40}}{\frac{30+20}{30+10+20+20}}$

we get

$P(B1|V) = \frac{\frac{30}{50}}{\frac{50}{80}}$

which gives us

$P(B1|V) = \frac{30}{80} * \frac{80}{50}$

we cancel out the 80s (replace them with 1s) and finally get:

$P(B1|V) = \frac{30}{50} = \frac{3}{5} = \frac{6}{10} = 0.6$

Similar explanations were found in the sub-chapter of the book.

* 2.2. Diachronic Bayes

Diachronic means 'related to change over time' (as we see evidence)

$P(A|B) = \frac{P(A) * P(B|A)}{P(B)}$

we replace A with H for hypothesis, and B with D for data and get:

$P(H|D) = \frac{P(H) * P(D|H)}{P(D)}$

where:

- P(H) - *prior*, prob of hypothesis before we see data
- P(H|D) - *posterior*, prob of hypothesis after we wee data
- P(D|H) - *likelihood*, prob of data under hypothesis
- P(D) - total prob of the data under any hypothesis

*prior* sometimes can be computed, but sometimes it is subjectively estimated.

total probability of data under any hypothesis is most difficult to calculate, but
we often simplify it by assuming that the hypothesis are:

- mutually exclusive (only one of them can be true)
- collectively exhaustive (one of them must be true)

In general we express it with the formula:

$P(D) = \sum_i P(H_i) * P(D|H_i)$

compare it with: 1.11. The Law of Total Probability.

The process in this section, using data and prior probability to compute a posterior probability is called a *Bayesian update*.

* 2.3. Bayes Tables

Bayes Tables are convenient tools for Bayesian update.

#+BEGIN_SRC

table = Dfs.DataFrame(Dict("bowl" => [1, 2]))
table.prior = [1/2, 1/2]
table.likelihood = [30/40, 20/40]
table.unnorm = table.prior .* table.likelihood

#+END_SRC

`unnorm` column, because those are 'unnormalized posteriors', each is a product of:

$P(H_i)*P(D|H_i)$

(see: numerator of the Bayes Theorem) after summing them up we get:

$P(D) = \sum_i P(D_i) * P(D|H_i)$

So the posterior is:

#+BEGIN_SRC

table.posterior = table.unnorm ./ sum(table.unnorm)

#+END_SRC
