#+TITLE: Ch02. Bayes’s Theorem
#+STARTUP: overview
#+STARTUP: indent
#+OPTIONS: \n: t

* Link to the chapter online
[[https://allendowney.github.io/ThinkBayes2/chap02.html][Chapter 2]]

* Warning
*The content of this file may be incorrect, erroneous and/or harmful. Use it at Your own risk.*

* Data Files
TODO
* Environment

#+BEGIN_SRC

println("Let's go.")

#+END_SRC

#+BEGIN_SRC

cd(joinpath(homedir(), "Desktop", "julia/luzne_zadanka/book4/ch02")

#+END_SRC

#+BEGIN_SRC

using Pkg
Pkg.activate(".")
Pkg.status()

#+END_SRC

* Imports

#+BEGIN_SRC

import DataFrames as Dfs

#+END_SRC

* Bayes's Theorem

$P(A|B) = \frac{P(A) * P(B|A)}{P(B)}$

* 2.1. The Cookie Problem

There are 2 bowls of cookies:

- Bowl 1 contains 30 vanilla cookies and 10 chocolate cookies.
- Bowl 2 contains 20 vanilla cookies and 20 chocolate cookies.

You choose a bowl at random and choose a cookie from it. The cookie is vanilla.
What is the probability that it came from Bowl1?

My (BL) solution (I solved it alone):

30v+20v = 50v

P(B1) = 30/50 = 3/5 = 6/10 = 0.6

P(B2) = 20/50 = 2/5 = 2/10 = 0.4

or from Beyes's Theorem

$P(A|B) = \frac{P(A) * P(B|A)}{P(B)}$

after replacing A with B1 and B with V, we get:

$P(B1|V) = \frac{P(B1) * P(V|B1)}{P(V)}$

and

$P(B1|V) = \frac{\frac{1}{2} * \frac{30}{40}}{\frac{30+20}{30+10+20+20}}$

we get

$P(B1|V) = \frac{\frac{30}{50}}{\frac{50}{80}}$

which gives us

$P(B1|V) = \frac{30}{80} * \frac{80}{50}$

we cancel out the 80s (replace them with 1s) and finally get:

$P(B1|V) = \frac{30}{50} = \frac{3}{5} = \frac{6}{10} = 0.6$

Similar explanations were found in the sub-chapter of the book.

* 2.2. Diachronic Bayes

Diachronic means 'related to change over time' (as we see evidence)

$P(A|B) = \frac{P(A) * P(B|A)}{P(B)}$

we replace A with H for hypothesis, and B with D for data and get:

$P(H|D) = \frac{P(H) * P(D|H)}{P(D)}$

where:

- P(H) - *prior*, prob of hypothesis before we see data
- P(H|D) - *posterior*, prob of hypothesis after we wee data
- P(D|H) - *likelihood*, prob of data under hypothesis
- P(D) - total prob of the data under any hypothesis

*prior* sometimes can be computed, but sometimes it is subjectively estimated.

total probability of data under any hypothesis is most difficult to calculate, but
we often simplify it by assuming that the hypothesis are:

- mutually exclusive (only one of them can be true)
- collectively exhaustive (one of them must be true)

In general we express it with the formula:

$P(D) = \sum_i P(H_i) * P(D|H_i)$

compare it with: 1.11. The Law of Total Probability.

The process in this section, using data and prior probability to compute a posterior probability is called a *Bayesian update*.

* 2.3. Bayes Tables

Bayes Tables are convenient tools for Bayesian update.

#+BEGIN_SRC

table = Dfs.DataFrame(Dict("bowl" => [1, 2]))
table.prior = [1/2, 1/2]
table.likelihood = [30/40, 20/40]
table.unnorm = table.prior .* table.likelihood

#+END_SRC

`unnorm` column, because those are 'unnormalized posteriors', each is a product of:

$P(H_i)*P(D|H_i)$

(see: numerator of the Bayes Theorem) after summing them up we get:

$P(D) = \sum_i P(D_i) * P(D|H_i)$

So the posterior is:

#+BEGIN_SRC

table.posterior = table.unnorm ./ sum(table.unnorm)

#+END_SRC

* 2.4. The Dice Problem

You have a box with a 6-, 8-, 12-sided dice. You choose one at random, roll it, and got 1.
What is the P(6-sided dice | 1)?

First a little helper function.

#+BEGIN_SRC

function rep(x::A, times::Int)::Vec{A} where A
    @assert times > 1 "times must be greater than 1"
    return [x for _ in 1:times]
end

#+END_SRC

Now the table with data.

#+BEGIN_SRC

table2 = Dfs.DataFrame(Dict("n sides on dice" => [6, 8, 12]))
table2.prior = rep(1//3, 3)
table2.likelihood = [1//6, 1//8, 1//12]

#+END_SRC

Now an update function.

#+BEGIN_SRC

function bayesUpdate!(df::Dfs.DataFrame)
    df.unnorm = df.prior .* df.likelihood
    df.posterior = df.unnorm ./ sum(df.unnorm)
    return nothing
end

#+END_SRC

And the update itself.

#+BEGIN_SRC

bayesUpdate!(table2)
table2

#+END_SRC

P(6-sided dice | 1) = 4//9

* 2.5. The Monty Hall Problem.

Three doors: 1, 2, 3. Prize behind one (a car). Goats behind other doors.  First
you pick Door 1. The host opens Door 3 and shows a goat. You may change your
choice now to Door 2. Would you? Is it worth it?

#+BEGIN_SRC

table3 = Dfs.DataFrame(Dict("Door" => 1:3))

#+END_SRC

P(H) - prob a car is behind Door 1, 2, 3.

#+BEGIN_SRC

table3.prior = rep(1//3, 3)

#+END_SRC

P(D|H) - prob host opened Door 3 if a car really is behind Door 1, 2, 3.

#+BEGIN_SRC

table3.likelihood = [1//2, 1, 0]

#+END_SRC

The update and result:

#+BEGIN_SRC

bayesUpdate!(table3)
table3

#+END_SRC

* 2.6. Summary

There's not much difference between using Bayes's Theorem and Bayes's table to
solve a problem like the ones presented in this chapter. Still, Bayes's table
makes it easier to compute the total probability of the data, especially for
problems with more than two hypothesis.

* 2.7. Exercises
** 2.7.1. Coins
You got 2 coins in a box, regular P(head) = 0.5, trick: P(head) = 1 (heads on
both sides).

You choose a coin at random and see head at one side. What is P(trick coin)?

#+BEGIN_SRC

df = Dfs.DataFrame(Dict("coin" => ["regular", "trick"]))
df.prior = rep(1//2, 2) # P(H)
df.likelihood = [1//2, 1] # P(D|H)
bayesUpdate!(df)
df

#+END_SRC

** 2.7.2. Siblings
Someone got two children. One of them is a girl. What is P(gg)?

#+BEGIN_SRC

df = Dfs.DataFrame(Dict("children" => ["gg", "gb or bg", "bb"]))
df.prior = [1//2 * 1//2, (1//2 * 1//2) * 2, 1//2 * 1//2] # P(H)
df.likelihood = [1, 1, 0] # P(D|H)
bayesUpdate!(df)
df

#+END_SRC

** 2.7.3. Monty Hall Modified
Monty always opens Door 2 if he can (if there is no car).

*** 2.7.3.1. Question 1
You choose Door 1, Monty opens Door 2, what is P(car behind Door 3)

#+BEGIN_SRC

df = Dfs.DataFrame(Dict("Car benind Door" => 1:3))
df.prior = rep(1//3, 3) # P(H)
df.likelihood = [1, 0, 1] # P(D|H)
bayesUpdate!(df)
df

#+END_SRC

*** 2.7.3.2. Question 2
You choose Door 1, Monty opens Door 3, what is P(car behind Door 2)

#+BEGIN_SRC

df = Dfs.DataFrame(Dict("Car benind Door" => 1:3))
df.prior = rep(1//3, 3) # P(H)
df.likelihood = [0, 1, 0] # P(D|H)
bayesUpdate!(df)
df

#+END_SRC

** 2.7.4. M&M's

In 1994, the colors M&M’s were: 30% Brown, 20% Yellow, 20% Red, 10% Green, 10% Orange, 10% Tan.
In 1996:  24% Blue , 20% Green, 16% Orange, 14% Yellow, 13% Red, 13% Brown.

You got two identical bags one from yr 1994, the other from 1996.  You pick one
from each at random.  Colors are: yellow and green. What is P(yellow was taken
from 1994 bag)?

#+BEGIN_SRC

df = Dfs.DataFrame(Dict("Bags order" => ["94_96", "96_94"]))
df.prior = rep(1//2, 2) # P(H)
# bag1, bag2 => yellow, green
# 1994: 20% yellow, 10% green
# 1996: 14% yellow, 20% green
df.likelihood = [0.2 * 0.2, 0.14 * 0.1] # P(D|H)
bayesUpdate!(df)
df

#+END_SRC
