#+TITLE: Ch05. Estimating Counts
#+STARTUP: overview
#+STARTUP: indent
#+OPTIONS: \n: t

* Link to the chapter online
[[https://allendowney.github.io/ThinkBayes2/chap05.html][Chapter 5]]

* Warning
*The content of this file may be incorrect, erroneous and/or harmful. Use it at Your own risk.*

* Data Files
None so far

* Environment

#+BEGIN_SRC

println("Let's go.")

#+END_SRC

#+BEGIN_SRC

cd(joinpath(homedir(), "Desktop", "julia/luzne_zadanka/book4/ch05")

#+END_SRC

#+BEGIN_SRC

using Pkg
Pkg.activate(".")
Pkg.status()

#+END_SRC

* Imports

#+BEGIN_SRC

# none so far

#+END_SRC

* Pmf
** struct definition

#+BEGIN_SRC

len(collection) = length(collection)

mutable struct Pmf{T}
    names::Vec{T} # names of hypotheses
    priors::Vec{Flt} # priors for each hypothesis
    likelihoods::Vec{Flt} # likelihoods for each hypothesis
    posteriors::Vec{Flt} # posteriors for each hypothesis

    Pmf(ns::Vec{Int}, prs) =
        (len(ns) != len(prs)) ?
        error("length(names) must be equal length(priors)") :
        new{Int}(
            ns, (prs ./ sum(prs)), zeros(len(ns)), zeros(len(ns))
        )

    Pmf(ns::Vec{Flt}, prs) =
        (len(ns) != len(prs)) ?
        error("length(names) must be equal length(priors)") :
        new{Flt}(
            ns, (prs ./ sum(prs)), zeros(len(ns)), zeros(len(ns))
        )
end

function round3(x::Flt)::Flt
    return round(x, digits=3)
end

function getFirstN(v::Vec{A}, n::Int)::Str where A
    @assert 0 < n <= 10 "n must be in range [1-10]"
    vv::Vec{A} = first(v, n)
    vv = eltype(v) == Flt ? round3.(vv) : vv
    result::Str = join(string.(vv), ", ")
    return len(v) > n ? result * "..." : result
end

function Base.show(io::IO, pmf::Pmf)
    result::Str = "names: $(getFirstN(pmf.names, 10))\n"
    result = result * "priors: $(getFirstN(pmf.priors, 10))\n"
    result = result * "likelihoods: $(getFirstN(pmf.likelihoods, 10))\n"
    result = result * "posteriors: $(getFirstN(pmf.posteriors, 10))\n"
    print(io, result)
    return nothing
end

#+END_SRC

** helper fns

#+BEGIN_SRC

function pmf2df(pmf::Pmf{A})::Dfs.DataFrame where A<:Ufi
    return Dfs.DataFrame(
        (;
        names=pmf.names,
        priors=pmf.priors,
        likelihoods=pmf.likelihoods,
        posteriors=pmf.posteriors
         )
    )
end

function getPmfFromSeq(seq::Vec{T})::Pmf{T} where T<:Ufi
    counts::Dict{T,Int} = Sb.countmap(seq)
    sortedKeys::Vec{T} = keys(counts) |> collect |> sort
    sortedVals::Vec{Int} = [counts[k] for k in sortedKeys]
    return Pmf(sortedKeys, sortedVals)
end

function getFieldValsEqName(pmf::Pmf{T}, name::T, fieldName::Str, default) where T<:Ufi
    ind = findfirst(x -> x == name, getproperty(pmf, Symbol("names")))
    return isnothing(ind) ? default : getproperty(pmf, Symbol(fieldName))[ind]
end

function getPriorByName(pmf::Pmf{T}, name::T)::Float64 where T<:Ufi
    return getFieldValsEqName(pmf, name, "priors", 0.0)
end

function getPriorsByNames(pmf::Pmf{T}, names::Vec{T})::Vec{Float64} where T<:Ufi
    return map(n -> getPriorByName(pmf, n), names)
end

function setLikelihoods!(pmf::Pmf{T}, newLikelihoods::Vec{Float64})::Pmf{T} where T<:Ufi
    pmf.likelihoods = newLikelihoods
    return pmf
end

function setPosteriors!(pmf::Pmf{T}, newPosteriors::Vec{Float64})::Pmf{T} where T<:Ufi
    pmf.posteriors = newPosteriors
    return pmf
end

"""
        normalizes pmf.posteriors so they add up to 1
"""
function normalizePosteriors!(pmf::Pmf{T})::Pmf{T} where T<:Ufi
    pmf.posteriors = pmf.posteriors ./ sum(pmf.posteriors)
    return pmf
end

"""
        updates posteriors (priors .* likeliehoods)
        if normalize = true, then posteriors are normalized
"""
function bayesUpdate!(pmf::Pmf{T}, normalize::Bool)::Pmf{T} where T<:Ufi
    setPosteriors!(pmf, pmf.priors .* pmf.likelihoods)
    if normalize
        normalizePosteriors!(pmf)
    end
    return pmf
end

function getIndMaxField(pmf::Pmf{T}, field::String)::Int where T<:Ufi
    maxProb::Float64 = max(getproperty(pmf, Symbol(field))...)
    return findfirst(x -> x == maxProb, getproperty(pmf, Symbol(field)))
end

function getIndMaxPosterior(pmf::Pmf)::Int
    return getIndMaxField(pmf, "posteriors")
end

function getIndMaxPrior(pmf::Pmf)::Int
    return getIndMaxField(pmf, "priors")
end

function getNameMaxPrior(pmf::Pmf{T})::T where T<:Ufi
    return pmf.names[getIndMaxPrior(pmf)]
end

function getNameMaxPosterior(pmf::Pmf{T})::T where T<:Ufi
    return pmf.names[getIndMaxPosterior(pmf)]
end

function updatePosteriors!(pmf::Pmf{<:Ufi},
                           nameThatOccurred::Ufi)::Pmf{<:Ufi}
    hypos::Vec{<:Ufi} = pmf.names
    likelihoods::Vec{Flt} = 1 ./ hypos
    impossible::Bv = nameThatOccurred .> hypos
    likelihoods[impossible] .= 0.0
    setLikelihoods!(pmf, likelihoods)
    if all(pmf.posteriors .== 0) # posteriors are initialized with 0s
        return bayesUpdate!(pmf, true)
    else
        pmf.posteriors .*= pmf.likelihoods
        return normalizePosteriors!(pmf)
    end
end

function updatePosteriors!(pmf::Pmf{<:Ufi},
                           namesThatOccurred::Vec{<:Ufi})::Pmf{<:Ufi}
    foreach(n -> updatePosteriors!(pmf, n), namesThatOccurred)
    return pmf
end

function updateBinomial!(pmf::Pmf{<:Ufi},
                           events::Vec{A},
                           eventsProbs::Dict{A, Vec{Flt}})::Pmf{<:Ufi} where A
    pmf.likelihoods .= 1
    for e in events
        pmf.likelihoods .*= eventsProbs[e]
    end
    return bayesUpdate!(pmf, true)
end

function updateBinomial!(pmf::Pmf{<:Ufi}, k::Int, n::Int)::Pmf{<:Ufi}
    @assert n >= k >= 0 "n and k must be >= 0"
    xs::Vec{<:Ufi} = pmf.names
    likelihoods::Vec{Flt} = Dst.pdf.(Dst.Binomial.(n, xs), k)
    setLikelihoods!(pmf, likelihoods)
    return bayesUpdate!(pmf, true)
end

function drawPmf(pmf::Pmf{T}, drawFn::Function,
    pmfFieldForYs::Str, title::Str, xlabel::Str, ylabel::Str)::Cmk.Figure where T<:Ufi
    fig = Cmk.Figure(size=(600, 400))
    drawFn(fig[1, 1],
        pmf.names, getproperty(pmf, Symbol(pmfFieldForYs)), color="navy",
        axis=(;
            title=title,
            xlabel=xlabel,
            ylabel=ylabel,
        ))
    return fig
end

function drawPriors(pmf::Pmf{T}, drawFn::Function,
                    title::Str, xlabel::Str, ylabel::Str)::Cmk.Figure where T<:Ufi
    return drawPmf(pmf, drawFn, "priors", title, xlabel, ylabel)
end

function drawPosteriors(pmf::Pmf{T}, drawFn::Function,
                        title::Str, xlabel::Str, ylabel::Str)::Cmk.Figure where T<:Ufi
    return drawPmf(pmf, drawFn, "posteriors", title, xlabel, ylabel)
end

function getPosteriorLE(pmf::Pmf{T}, cutoffName::T)::Flt where T<:Ufi
    inds::Vec{Int} = findall((<=)(cutoffName), pmf.names)
    return sum(pmf.posteriors[inds])
end

function getPosteriorGT(pmf::Pmf{T}, cutoffName::T)::Flt where T<:Ufi
    inds::Vec{Int} = findall((>)(cutoffName), pmf.names)
    return sum(pmf.posteriors[inds])
end

function getPosteriorQuantile(pmf::Pmf{T}, prob::Flt)::Flt where T<:Ufi
    @assert 0 <= prob <= 1 "prob must be in range [0-1]"
    totalProb::Flt = 0
    for i in eachindex(pmf.posteriors)
        totalProb += pmf.posteriors[i]
        if totalProb >= prob
            return pmf.names[i]
        end
    end
    return -99.0
end

function getPosteriorCredibleInterval(pmf::Pmf{T}, prob::Flt)::Vec{Flt} where T<:Ufi
    intervalLow::Flt = 0.5 - prob/2
    intervalHigh::Flt = 0.5 + prob/2
    return getPosteriorQuantile.(Ref(pmf), [intervalLow, intervalHigh])
end

function drawPriorsPosteriors(pmf::Pmf{<:Ufi}, drawFn!::Function,
                              title::Str, xlabel::Str, ylabel::Str)::Cmk.Figure
    fig::Cmk.Figure = Cmk.Figure()
    ax::Cmk.Axis = Cmk.Axis(fig[1, 1], title=title, xlabel=xlabel, ylabel=ylabel)
    drawFn!(ax, pmf.names, pmf.priors, label="prior")
    drawFn!(ax, pmf.names, pmf.posteriors, label="posteriors")
    Cmk.axislegend();
    return fig
end

function drawProbsComparison(pmf1::Pmf{<:Ufi}, pmf2::Pmf{<:Ufi}, fieldName::Str,
                             drawFn!::Function, label1::Str, label2::Str,
                             title::Str, xlabel::Str, ylabel::Str)::Cmk.Figure
    fig::Cmk.Figure = Cmk.Figure()
    ax::Cmk.Axis = Cmk.Axis(fig[1, 1], title=title, xlabel=xlabel, ylabel=ylabel)
    ys1::Vec{Flt} = getproperty(pmf1, Symbol(fieldName))
    ys2::Vec{Flt} = getproperty(pmf2, Symbol(fieldName))
    drawFn!(ax, pmf1.names, ys1, label=label1)
    drawFn!(ax, pmf2.names, ys2, label=label2)
    Cmk.axislegend();
    return fig
end

#+END_SRC

** other fns

#+BEGIN_SRC

function rep(x::A, times::Int)::Vec{A} where A
    @assert times > 1 "times must be greater than 1"
    return [x for _ in 1:times]
end

function rep(v::Vec{A}, times::Vec{Int})::Vec{A} where A
    @assert (len(v) == len(times)) "length(v) not equal length(times)"
    @assert all(map((>)(0), times)) "times elts must be GT 0"
    result::Vec{A} = Vec{eltype(v)}(undef, sum(times))
    currInd::Int = 1
    for i in eachindex(v)
        for _ in 1:times[i]
            result[currInd] = v[i]
            currInd += 1
        end
    end
    return result
end

c(sth) = collect(sth)
str(sth) = string(sth)

#+END_SRC

* 5.1. The Train Problem

The problem from Frederick Mosteller’s [[https://store.doverpublications.com/products/9780486653556][Fifty Challenging Problems in Probability with Solutions]]:

“A railroad numbers its locomotives in order 1…N. One day you see a locomotive with the number 60. Estimate how many locomotives the railroad has.”

Pmf:

#+BEGIN_SRC

train = getPmfFromSeq(1:1000 |> c)

#+END_SRC

Update:

#+BEGIN_SRC

updatePosteriors!(train, 60)

#+END_SRC

Posteriors:

#+BEGIN_SRC

drawPosteriors(train, Cmk.lines,
               "Posterior distribution\nafter locomative no. 60 was observed",
               "Number of trains", "PMF")

#+END_SRC

Max Posterior:

#+BEGIN_SRC

maxPostInd = getIndMaxPosterior(train)
train.names[maxPostInd], train.posteriors[maxPostInd]

#+END_SRC

*maxPostInd - maximizes the chance of getting the answer exactly right.*

What is the mean posterior destribution, i.e.

$mean = \sum_i (p_i * q_i)$

where:

- $q_i$ - a set of possible quantities
- $p_i$ - a set of possible probabilities for the quantities

#+BEGIN_SRC

sum(train.names .* train.posteriors)

#+END_SRC

Of course we can close it to a function (see PMF section) and use it like so:

#+BEGIN_SRC

getMeanPosteriorName(train)

#+END_SRC

*getMeanPosteriorName - minimizes the mean squared error* over a long run.

* 5.2. Sensitivity to the Prior

With small data set of observations the posterior is sensitive to the prior.

#+BEGIN_SRC

upBonds = [500, 1000, 2000]
posteriorMeans = Flt[]

for upBond in upBonds
    train = getPmfFromSeq(1:upBond |> c)
    updatePosteriors!(train, 60)
    push!(posteriorMeans, round3(getMeanPosteriorName(train)))
end

Dfs.DataFrame("upper bond" => upBonds, "posterior mean" => posteriorMeans)

#+END_SRC

With more data, the posteriors tend to converge, e.g. let's say we observed
trains: 60, 30, and 90.

#+BEGIN_SRC

upBonds = [500, 1000, 2000]
posteriorMeans = Flt[]
trainNums = [60, 30, 90]

for upBond in upBonds
    train = getPmfFromSeq(1:upBond |> c)
    updatePosteriors!(train, trainNums)
    push!(posteriorMeans, round3(getMeanPosteriorName(train)))
end

Dfs.DataFrame("upper bond" => upBonds, "posterior mean" => posteriorMeans)

#+END_SRC

Compare the above with section 4.4. Triangle Prior.

* 5.3. Power Law Prior

If more data are not available we may try to improve prior by gathering more
background information.

In fact, the distribution of company sizes tends to follow a power law, as
Robert Axtell reports in Science
(http://www.sciencemag.org/content/293/5536/1818.full.pdf).

Mathematically this can be expressed as:

$N \propto (\frac{1}{N})^{\alpha}$

where:

- $\alpha$ is a parameter that is often near 1

Since from mathematics we know that:

$a^{-n} = \frac{1}{a^{n}}$

So we can rewrite the above as:

$N \propto N^{-\alpha}$

Once we know that we may compare the impact of both priors (uniform vs. power
law) on the posteriors.

#+BEGIN_SRC

alpha = 1.0
nLocomotives = 1:1000 |> c
trainPowerLawPriors = Pmf(nLocomotives, nLocomotives.^(-alpha))
trainUniformPriors = getPmfFromSeq(nLocomotives)

#+END_SRC

First, let's see the priors:

#+BEGIN_SRC

drawProbsComparison(trainPowerLawPriors, trainUniformPriors, "priors",
                    Cmk.lines!, "power law", "uniform",
                    "Prior distributions", "Number of trains", "PMF")

#+END_SRC

Now, the update:

#+BEGIN_SRC

dataset = [60]
updatePosteriors!(trainPowerLawPriors, dataset)
updatePosteriors!(trainUniformPriors, dataset)

#+END_SRC

And posteriors:

#+BEGIN_SRC

drawProbsComparison(trainPowerLawPriors, trainUniformPriors, "posteriors",
                    Cmk.lines!, "power law", "uniform",
                    "Prior distributions", "Number of trains", "PMF")

#+END_SRC

Now, let's see how the better priors translate into posterior means.
First, a small dataset:

#+BEGIN_SRC

upBonds = [500, 1000, 2000]
alpha = 1.0
posteriorMeans = Flt[]
# small data set
trainNums = [60]

for upBond in upBonds
    nTrains = 1:upBond |> c
    train = Pmf(nTrains, nTrains.^(-alpha))
    updatePosteriors!(train, trainNums)
    push!(posteriorMeans, round3(getMeanPosteriorName(train)))
end

Dfs.DataFrame("upper bond" => upBonds, "posterior mean" => posteriorMeans)

#+END_SRC

Next, a larger dataset:

#+BEGIN_SRC

upBonds = [500, 1000, 2000]
alpha = 1.0
posteriorMeans = Flt[]
# larger data set 60
trainNums = [30, 60, 90]

for upBond in upBonds
    nTrains = 1:upBond |> c
    train = Pmf(nTrains, nTrains.^(-alpha))
    updatePosteriors!(train, trainNums)
    push!(posteriorMeans, round3(getMeanPosteriorName(train)))
end

Dfs.DataFrame("upper bond" => upBonds, "posterior mean" => posteriorMeans)

#+END_SRC

Compare with Section 5.2. Sensitivity to the Prior. The spread of posterior
means is (much) smaller.

* 5.4. Credible Intervals

Other way to summarize a posterior distribution (except of point estimates we
met so far) is with percentiles.

For instance (see the PMF section above) we may calculate the posterior probability that a company has less than or equal to 100 trains:

#+BEGIN_SRC

alpha = 1.0
nLocomotives = 1:1000 |> c
trainPowerLawPriors = Pmf(nLocomotives, nLocomotives.^(-alpha))
dataset = [60, 30, 90]
updatePosteriors!(trainPowerLawPriors, dataset)

#+END_SRC

and calculations:

#+BEGIN_SRC

getPosteriorLE(trainPowerLawPriors, 100)

#+END_SRC

We can also calculate the quantile (see the PMF section above):

#+BEGIN_SRC

getPosteriorQuantile(trainPowerLawPriors, 0.5)

#+END_SRC

or a 90% confidence interval for the number of trains:

#+BEGIN_SRC

getPosteriorQuantile.(Ref(trainPowerLawPriors), [0.05, 0.95])
getPosteriorCredibleInterval(trainPowerLawPriors, 0.9)

#+END_SRC

* 5.5. The German Tank Problem

Reasoning similar to the one presented in this chapter was used to solve [[https://en.wikipedia.org/wiki/German_tank_problem][the
German Tank Problem]] during WW2.

* 5.6. Informative Priors

Among Bayesians there are two approaches to choosing prior distributions:

- informative (based on the background information about the problem),
- uninformative (as unrestricted as possible)

Both got priors and cons, for instance in the spectrum of
objectiveness-subjectiveness.

If you have a lot of data then the choice of the prior doesn't really matter.
However, with a limited amount of data you will likely to get different results
(compare the train case with uniform priors and power law priors).

My impression is that the author of the book inclines rather towards informative
priors.

* 5.7. Summary

This chapter focused on estimating counts (or the size of a population).

In the next chapter we will talk about the odds, as an alternative way to
express probabilities, and Bayes Rule as an alternative form of Bayes's Theorem.

* 5.8. Exercises
** 5.8.1. Exercise 1

In a lecture hall the safe capacity of the room is 1200 people, so you want to
estimate how many people are there, therefore:

- You ask how many people were born on May 11 and two people raise their hands.
- You ask how many were born on May 23 and 1 person raises their hand.
- Finally, you ask how many were born on August 1, and no one raises their hand.

How many people are in the audience? What is the probability that there are more
than 1200 people. Hint: Remember the binomial distribution.

Priors:

#+BEGIN_SRC

# 1200 is our reference point
# let's say a person can tell the difference
# when there is half less/more people than expected
# so we will test the range of people:
lecture = getPmfFromSeq(c(600:1800))

#+END_SRC

#+BEGIN_SRC

# for binom distribution calculations
# assumption: prob that a person was born on a given day is 1/365
# assumption: equal distrib of births through year
# assumption: independence of birth dates and their probabilities
# so, overall we got k successes, i.e. 2 AND 1 AND 0
# p = 1/365
# n - number of people in the lecture hall
p = 1/365
ks = [2, 1, 0]
ns = lecture.names

#+END_SRC

Update:

#+BEGIN_SRC

likelihoods = ones(Flt, lecture.names |> len)
for k in ks
    likelihoods .*= Dst.pdf.(Dst.Binomial.(ns, p), k)
end
setLikelihoods!(lecture, likelihoods)
bayesUpdate!(lecture, true)

#+END_SRC

Visual inspection:

#+BEGIN_SRC

drawPriorsPosteriors(lecture, Cmk.lines!,
                     "Lecture hall", "number of people in the room", "PMF")

#+END_SRC

Most probable number of people in the hall:

#+BEGIN_SRC

indMax = getIndMaxPosterior(lecture)
lecture.names[indMax], lecture.posteriors[indMax]

#+END_SRC

95% CI:

#+BEGIN_SRC

getPosteriorCredibleInterval(lecture, 0.95)

#+END_SRC

Prob n people in the hall > 1200:

#+BEGIN_SRC

getPosteriorGT(lecture, 1200)

#+END_SRC

** 5.8.2. Exercise 2
There are rabbits in a garden behind a house. And there is a camera that takes a
picture of the first rabbit it seas in a given day. After three days there are
three pictures, two of them are the pictures of the same rabbit, the other one
is of different rabbit. How many rabbits are in the garden?

Choose uniform prior from 4 to 10.
All rabbits are equally likely to be photographed.

First the rabbits:

#+BEGIN_SRC

rabbits = getPmfFromSeq(c(4:10))

#+END_SRC

Reasoning for likelihood:

if:

- rx - rabbit x that occurred twice
- aor - any other rabbit that occurred only once

Possible distributions of rabbits in the 3 days:

- rx, rx, aor
- rx, aor, rx
- aor, rx, rx

for option1 (rx, rx, aor) where n - total number of rabbits:

- P(rx) = 1/n AND P(rx) = 1/n AND P(aor) = (n-1)/n

other options the same calculations but in different order (so P(option1)*3)

And so we got:

$ \frac{1}{n} * \frac{1}{n} * \frac{n-1}{n} = \frac{1 * 1 * (n-1)}{n * n * n} $

Which gives us:

$ \frac{n - 1}{n^3} $

So the likelihood function is:

#+BEGIN_SRC

function getRabbitLikelihood(nRabbits::Int)::Flt
    @assert 4 <= nRabbits <= 10 "nRabbits must be in range [4-10]"
    return (nRabbits-1)/(nRabbits^3) * 3
end

#+END_SRC

The likelihoods and the update are:

#+BEGIN_SRC

setLikelihoods!(rabbits, getRabbitLikelihood.(rabbits.names))
bayesUpdate!(rabbits, true)

#+END_SRC

And its graphical depiction:

#+BEGIN_SRC

drawPriorsPosteriors(rabbits, Cmk.lines!,
                     "Rabbits in the garden with the camera phototrap",
                     "number of rabbits", "PMF")

#+END_SRC

And another graphics:

#+BEGIN_SRC

drawPosteriors(rabbits, Cmk.barplot,
               "Rabbits in the garden with the camera phototrap",
               "number of rabbits", "PMF")

#+END_SRC

** 5.8.3. Exercise 3

In a criminal justice system, all prison sentences are either 1, 2, or 3 years,
with an equal number of each. One day, you visit a prison and choose a prisoner
at random. What is the probability that they are serving a 3-year sentence? What
is the average remaining sentence of the prisoners you observe?

*** Solution 1

Computer simulation:

#+BEGIN_SRC

mutable struct Prisoner
    sentenceYrs::Int
    timeLeftYrs::Int
end

prisoners = Prisoner.(1:3, 1:3)

# actually a 3-year cycle would be enough, but whatever
for _ in 1:1000
    foreach(p -> p.timeLeftYrs -= 1, prisoners)
    filter!(p -> p.timeLeftYrs > 0, prisoners)
    append!(prisoners, Prisoner.(1:3, 1:3))
end

#+END_SRC

Probability that a prisoner is serving x-year sentence:

#+BEGIN_SRC

[p.sentenceYrs for p in prisoners] |> Sb.proportionmap

#+END_SRC

Average remaining sentence:

#+BEGIN_SRC

sum([p.timeLeftYrs for p in prisoners])/len(prisoners)

#+END_SRC

*** Solution 2
The posteriors should not be uniform if the equal number of 1-, 2-, 3- year
sentences is made each year.

Explanation:

- we start with: 1, 2, 3, (1, 2, 3 years remained),
- after 1 yr we release the old 1yr and got: 2 (1 yr remained), 3 (2 years remained) + newly sentenced: 1, 2, 3 (1, 2, 3 years remained),
- after another yr we reduce the time left and again add newly sentenced.

(compare with Solution 1 above)

So at any moment there should be more 3years than 2 years and 1 years (3 > 2 > 1).

#+BEGIN_SRC

prisoners = getPmfFromSeq(c(1:3))
# likelihood proportional to the sentence
setLikelihoods!(prisoners, prisoners.names ./ sum(prisoners.names))
bayesUpdate!(prisoners, true)

#+END_SRC

