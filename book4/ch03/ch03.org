#+TITLE: Ch03. Distributions
#+STARTUP: overview
#+STARTUP: indent
#+OPTIONS: \n: t

* Link to the chapter online
[[https://allendowney.github.io/ThinkBayes2/chap03.html][Chapter 3]]

* Warning
*The content of this file may be incorrect, erroneous and/or harmful. Use it at Your own risk.*

* Data Files
none yet
* Environment

#+BEGIN_SRC

println("Let's go.")

#+END_SRC

#+BEGIN_SRC

cd(joinpath(homedir(), "Desktop", "julia/luzne_zadanka/book4/ch03")

#+END_SRC

#+BEGIN_SRC

using Pkg
Pkg.activate(".")
Pkg.status()

#+END_SRC

* Imports

#+BEGIN_SRC

import DataFrames as Dfs

#+END_SRC

* Pmf
** struct definition

#+BEGIN_SRC

mutable struct Pmf{T}
    names::Vector{T} # names of hypotheses
    priors::Vector{Flt} # priors for each hypothesis
    likelihoods::Vector{Flt} # likelihoods for each hypothesis
    posteriors::Vector{Flt} # posteriors for each hypothesis

    Pmf(ns::Vector{Int}, prs) =
        (length(ns) != length(prs)) ?
        error("length(names) must be equal length(priors)") :
        new{Int}(
            ns, (prs ./ sum(prs)), zeros(length(ns)), zeros(length(ns))
        )

    Pmf(ns::Vector{Flt}, prs) =
        (length(ns) != length(prs)) ?
        error("length(names) must be equal length(priors)") :
        new{Flt}(
            ns, (prs ./ sum(prs)), zeros(length(ns)), zeros(length(ns))
        )

    Pmf(ns::Vector{Str}, prs) =
        (length(ns) != length(prs)) ?
        error("length(names) must be equal length(priors)") :
        new{Str}(
            ns, (prs ./ sum(prs)), zeros(length(ns)), zeros(length(ns))
        )
end

function Base.show(io::IO, pmf::Pmf)
    trim::Bool = length(pmf.names) > 10
    result::Str = "names: $(join(trim ? pmf.names[1:10] : pmf.names, ", "))$(trim ? ", ..." : "")\n"
    result = result * "priors: $(join(map(x -> round(x, digits=3) |> string, trim ? pmf.priors[1:10] : pmf.priors), ", "))$(trim ? ", ..." : "")\n"
    result = result * "likelihoods: $(join(map(x -> round(x, digits=3) |> string, trim ? pmf.likelihoods[1:10] : pmf.likelihoods), ", "))$(trim ? ", ..." : "")\n"
    result = result * "posteriors: $(join(map(x -> round(x, digits=3) |> string, trim ? pmf.posteriors[1:10] : pmf.posteriors),  ", "))$(trim ? ", ..." : "")\n"
    print(io, result)
end

#+END_SRC

** helper fns

#+BEGIN_SRC

function pmf2df(pmf::Pmf{T})::Dfs.DataFrame where {T}
    return Dfs.DataFrame(
        (;
        names=pmf.names,
        priors=pmf.priors,
        likelihoods=pmf.likelihoods,
        posteriors=pmf.posteriors
         )
    )
end

function getCounts(v::Vector{T})::Dict{T,Int} where {T}
    result::Dict{T,Int} = Dict()
    for elt in v
        result[elt] = get(result, elt, 0) + 1
    end
    return result
end

function getPmfFromSeq(seq::Vector{T})::Pmf{T} where {T}
    counts::Dict{T,Int} = getCounts(seq)
    sortedKeys::Vector{T} = keys(counts) |> collect |> sort
    sortedVals::Vector{Int} = [counts[k] for k in sortedKeys]
    return Pmf(sortedKeys, sortedVals)
end
function getFieldValsEqName(pmf::Pmf{T}, name::T, fieldName::String, default) where {T}
    ind = findfirst(x -> x == name, getproperty(pmf, Symbol("names")))
    return isnothing(ind) ? default : getproperty(pmf, Symbol(fieldName))[ind]
end

function getPriorByName(pmf::Pmf{T}, name::T)::Float64 where {T}
    return getFieldValsEqName(pmf, name, "priors", 0.0)
end

function getPriorsByNames(pmf::Pmf{T}, names::Vector{T})::Vector{Float64} where {T}
    return map(n -> getPriorByName(pmf, n), names)
end

function setLikelihoods!(pmf::Pmf{T}, newLikelihoods::Vector{Float64}) where {T}
    pmf.likelihoods = newLikelihoods
end

function setPosteriors!(pmf::Pmf{T}, newPosteriors::Vector{Float64}) where {T}
    pmf.posteriors = newPosteriors
end

"""
        normalizes pmf.posteriors so they add up to 1
"""
function normalizePosteriors!(pmf::Pmf{T}) where {T}
    pmf.posteriors = pmf.posteriors ./ sum(pmf.posteriors)
end

"""
        updates posteriors (priors .* likeliehoods)
        if normalize = true, then posteriors are normalized
"""
function bayesUpdate!(pmf::Pmf{T}, normalize::Bool=true) where {T}
    setPosteriors!(pmf, pmf.priors .* pmf.likelihoods)
    if normalize
        normalizePosteriors!(pmf)
    end
end

#+END_SRC

** other fns (mostly one-liners)

#+BEGIN_SRC

function rep(x::A, times::Int)::Vec{A} where A
    @assert times > 1 "times must be greater than 1"
    return [x for _ in 1:times]
end

c(sth) = collect(sth)
str(sth) = string(sth)

#+END_SRC

* 3.1. Distributions

a distribution - a set of possible outcomes and their corresponding probabilities

* 3.2. Probability Mass Functions

PMF - maps each possible outcome to its probability (for discrete distributions).

#+BEGIN_SRC

coin = Pmf(["heads", "tails"], rep(1/2, 2))
coin |> pmf2df

#+END_SRC

#+BEGIN_SRC

die = getPmfFromSeq(c(1:6))
die |> pmf2df

#+END_SRC

#+BEGIN_SRC

letters = getPmfFromSeq(str.(c("Mississippi")))
letters |> pmf2df

#+END_SRC

#+BEGIN_SRC

getPriorByName(letters, "s")
getPriorByName(letters, "t")

#+END_SRC


#+BEGIN_SRC

getPriorsByNames(die, [1, 4, 7])

#+END_SRC

* 3.3. The Cookie Problem Revisited

There are 2 bowls of cookies:

- Bowl 1 contains 30 vanilla cookies and 10 chocolate cookies.
- Bowl 2 contains 20 vanilla cookies and 20 chocolate cookies.

You choose a bowl at random and choose a cookie from it. The cookie is vanilla.
What is the probability that it came from Bowl1?

#+BEGIN_SRC

cookies = getPmfFromSeq(["Bowl 1", "Bowl 2"])
cookies |> pmf2df

#+END_SRC

#+BEGIN_SRC

setLikelihoods!(cookies, [30/40, 20/40])
cookies |> pmf2df

#+END_SRC

#+BEGIN_SRC

bayesUpdate!(cookies)
cookies |> pmf2df

#+END_SRC

You put the cookie back to the bowl and draw again from the same bowl.
The second cookie is also vanilla. What are the posteriors now?

#+BEGIN_SRC

cookies.posteriors .*= cookies.likelihoods
normalizePosteriors!(cookies)
cookies |> pmf2df

#+END_SRC

We do the same thing again, but this time we get a chocolate cookie.
What are the posteriors now?

#+BEGIN_SRC

setLikelihoods!(cookies, [10/40, 20/40])
cookies.posteriors .*= cookies.likelihoods
normalizePosteriors!(cookies)
cookies |> pmf2df

#+END_SRC

* 3.4. 101 Bowls

Cookie problem with 101 Bowls (vanilla and/or chocolate cookies):

- Bowl 1 contains 0% vanilla cookies,
- Bowl 2 contains 1% vanilla cookies,
- Bowl 99 contains 99% vanilla cookies,
- Bowl 100 contains 100% vanilla cookies,

You choose a bowl at random and choose a cookie from it. The cookie is vanilla.
What is the probability that it came from Bowl x?

#+BEGIN_SRC

bowls101 = getPmfFromSeq(c(0:100))
bowls101.likelihoods = bowls101.names ./ 100
bayesUpdate!(bowls101)

fig = Cmk.Figure();
ax = Cmk.Axis(fig[1, 1],
              title="Posterior after one vanilla cookie",
              xlabel="Bowl #", ylabel="PMF",
              xticks=(0:20:100), yticks=(0:0.0025:0.02)
              );
Cmk.lines!(ax, bowls101.names, bowls101.posteriors, color=:blue, label="posterior");
Cmk.lines!(ax, bowls101.names, bowls101.priors, color=:gray, label="prior");
fig

#+END_SRC
