#+TITLE: Ch05. Estimating Counts
#+STARTUP: overview
#+STARTUP: indent
#+OPTIONS: \n: t

* Link to the chapter online
[[https://allendowney.github.io/ThinkBayes2/chap05.html][Chapter 5]]

* Warning
*The content of this file may be incorrect, erroneous and/or harmful. Use it at Your own risk.*

* Data Files
None so far

* Environment

#+BEGIN_SRC

println("Let's go.")

#+END_SRC

#+BEGIN_SRC

cd(joinpath(homedir(), "Desktop", "julia/luzne_zadanka/book4/ch05")

#+END_SRC

#+BEGIN_SRC

using Pkg
Pkg.activate(".")
Pkg.status()

#+END_SRC

* Imports

#+BEGIN_SRC

# none so far

#+END_SRC

* Pmf
** struct definition

#+BEGIN_SRC

len(collection) = length(collection)

mutable struct Pmf{T}
    names::Vec{T} # names of hypotheses
    priors::Vec{Flt} # priors for each hypothesis
    likelihoods::Vec{Flt} # likelihoods for each hypothesis
    posteriors::Vec{Flt} # posteriors for each hypothesis

    Pmf(ns::Vec{Int}, prs) =
        (len(ns) != len(prs)) ?
        error("length(names) must be equal length(priors)") :
        new{Int}(
            ns, (prs ./ sum(prs)), zeros(len(ns)), zeros(len(ns))
        )

    Pmf(ns::Vec{Flt}, prs) =
        (len(ns) != len(prs)) ?
        error("length(names) must be equal length(priors)") :
        new{Flt}(
            ns, (prs ./ sum(prs)), zeros(len(ns)), zeros(len(ns))
        )
end

function round3(x::Flt)::Flt
    return round(x, digits=3)
end

function getFirstN(v::Vec{A}, n::Int)::Str where A
    @assert 0 < n <= 10 "n must be in range [1-10]"
    vv::Vec{A} = first(v, n)
    vv = eltype(v) == Flt ? round3.(vv) : vv
    result::Str = join(string.(vv), ", ")
    return len(v) > n ? result * "..." : result
end

function Base.show(io::IO, pmf::Pmf)
    result::Str = "names: $(getFirstN(pmf.names, 10))\n"
    result = result * "priors: $(getFirstN(pmf.priors, 10))\n"
    result = result * "likelihoods: $(getFirstN(pmf.likelihoods, 10))\n"
    result = result * "posteriors: $(getFirstN(pmf.posteriors, 10))\n"
    print(io, result)
    return nothing
end

#+END_SRC

** helper fns

#+BEGIN_SRC

function pmf2df(pmf::Pmf{A})::Dfs.DataFrame where A<:Ufi
    return Dfs.DataFrame(
        (;
        names=pmf.names,
        priors=pmf.priors,
        likelihoods=pmf.likelihoods,
        posteriors=pmf.posteriors
         )
    )
end

function getCounts(v::Vec{T})::Dict{T,Int} where {T}
    result::Dict{T,Int} = Dict()
    for elt in v
        result[elt] = get(result, elt, 0) + 1
    end
    return result
end

function getPmfFromSeq(seq::Vec{T})::Pmf{T} where T<:Ufi
    counts::Dict{T,Int} = getCounts(seq)
    sortedKeys::Vec{T} = keys(counts) |> collect |> sort
    sortedVals::Vec{Int} = [counts[k] for k in sortedKeys]
    return Pmf(sortedKeys, sortedVals)
end

function getFieldValsEqName(pmf::Pmf{T}, name::T, fieldName::Str, default) where T<:Ufi
    ind = findfirst(x -> x == name, getproperty(pmf, Symbol("names")))
    return isnothing(ind) ? default : getproperty(pmf, Symbol(fieldName))[ind]
end

function getPriorByName(pmf::Pmf{T}, name::T)::Float64 where T<:Ufi
    return getFieldValsEqName(pmf, name, "priors", 0.0)
end

function getPriorsByNames(pmf::Pmf{T}, names::Vec{T})::Vec{Float64} where T<:Ufi
    return map(n -> getPriorByName(pmf, n), names)
end

function setLikelihoods!(pmf::Pmf{T}, newLikelihoods::Vec{Float64})::Pmf{T} where T<:Ufi
    pmf.likelihoods = newLikelihoods
    return pmf
end

function setPosteriors!(pmf::Pmf{T}, newPosteriors::Vec{Float64})::Pmf{T} where T<:Ufi
    pmf.posteriors = newPosteriors
    return pmf
end

"""
        normalizes pmf.posteriors so they add up to 1
"""
function normalizePosteriors!(pmf::Pmf{T})::Pmf{T} where T<:Ufi
    pmf.posteriors = pmf.posteriors ./ sum(pmf.posteriors)
    return pmf
end

"""
        updates posteriors (priors .* likeliehoods)
        if normalize = true, then posteriors are normalized
"""
function bayesUpdate!(pmf::Pmf{T}, normalize::Bool)::Pmf{T} where T<:Ufi
    setPosteriors!(pmf, pmf.priors .* pmf.likelihoods)
    if normalize
        normalizePosteriors!(pmf)
    end
    return pmf
end

function getIndMaxField(pmf::Pmf{T}, field::String)::Int where T<:Ufi
    maxProb::Float64 = max(getproperty(pmf, Symbol(field))...)
    return findfirst(x -> x == maxProb, getproperty(pmf, Symbol(field)))
end

function getIndMaxPosterior(pmf::Pmf)::Int
    return getIndMaxField(pmf, "posteriors")
end

function getIndMaxPrior(pmf::Pmf)::Int
    return getIndMaxField(pmf, "priors")
end

function getNameMaxPrior(pmf::Pmf{T})::T where T<:Ufi
    return pmf.names[getIndMaxPrior(pmf)]
end

function getNameMaxPosterior(pmf::Pmf{T})::T where T<:Ufi
    return pmf.names[getIndMaxPosterior(pmf)]
end

function updatePosteriors!(pmf::Pmf{<:Ufi},
                           nameThatOccurred::Ufi)::Pmf{<:Ufi}
    hypos::Vec{<:Ufi} = pmf.names
    likelihoods::Vec{Flt} = 1 ./ hypos
    impossible::Bv = nameThatOccurred .> hypos
    likelihoods[impossible] .= 0.0
    setLikelihoods!(pmf, likelihoods)
    if all(pmf.posteriors .== 0) # posteriors are initialized with 0s
        return bayesUpdate!(pmf, true)
    else
        pmf.posteriors .*= pmf.likelihoods
        return normalizePosteriors!(pmf)
    end
end

function updatePosteriors!(pmf::Pmf{<:Ufi},
                           namesThatOccurred::Vec{<:Ufi})::Pmf{<:Ufi}
    foreach(n -> updatePosteriors!(pmf, n), namesThatOccurred)
    return pmf
end

function updateBinomial!(pmf::Pmf{<:Ufi},
                           events::Vec{A},
                           eventsProbs::Dict{A, Vec{Flt}})::Pmf{<:Ufi} where A
    pmf.likelihoods .= 1
    for e in events
        pmf.likelihoods .*= eventsProbs[e]
    end
    return bayesUpdate!(pmf, true)
end

function updateBinomial!(pmf::Pmf{<:Ufi}, k::Int, n::Int)::Pmf{<:Ufi}
    @assert n >= k >= 0 "n and k must be >= 0"
    xs::Vec{<:Ufi} = pmf.names
    likelihoods::Vec{Flt} = Dst.pdf.(Dst.Binomial.(n, xs), k)
    setLikelihoods!(pmf, likelihoods)
    return bayesUpdate!(pmf, true)
end

function drawLinesPmf(pmf::Pmf{T},
    pmfFieldForYs::Str, title::Str, xlabel::Str, ylabel::Str)::Cmk.Figure where T<:Ufi
    fig = Cmk.Figure(size=(600, 400))
    Cmk.lines(fig[1, 1],
        pmf.names, getproperty(pmf, Symbol(pmfFieldForYs)), color="navy",
        axis=(;
            title=title,
            xlabel=xlabel,
            ylabel=ylabel,
        ))
    return fig

end

function drawPriors(pmf::Pmf{T}, title::Str, xlabel::Str, ylabel::Str)::Cmk.Figure where T<:Ufi
    return drawLinesPmf(pmf, "priors", title, xlabel, ylabel)
end

function drawPosteriors(pmf::Pmf{T}, title::Str, xlabel::Str, ylabel::Str)::Cmk.Figure where T<:Ufi
    return drawLinesPmf(pmf, "posteriors", title, xlabel, ylabel)
end

function getPosteriorLE(pmf::Pmf{T}, cutoffName::T)::Flt where T<:Ufi
    inds::Vec{Int} = findall((<=)(cutoffName), pmf.names)
    return sum(pmf.posteriors[inds])
end

function getPosteriorQuantile(pmf::Pmf{T}, prob::Flt)::Flt where T<:Ufi
    @assert 0 <= prob <= 1 "prob must be in range [0-1]"
    totalProb::Flt = 0
    for i in eachindex(pmf.posteriors)
        totalProb += pmf.posteriors[i]
        if totalProb >= prob
            return pmf.names[i]
        end
    end
    return -99.0
end

function getPosteriorCredibleInterval(pmf::Pmf{T}, prob::Flt)::Vec{Flt} where T<:Ufi
    intervalLow::Flt = 0.5 - prob/2
    intervalHigh::Flt = 0.5 + prob/2
    return getPosteriorQuantile.(Ref(pmf), [intervalLow, intervalHigh])
end

function drawProbsComparison(pmf1::Pmf{<:Ufi}, pmf2::Pmf{<:Ufi}, fieldName::Str,
                             label1::Str, label2::Str, title::Str,
                             xlabel::Str, ylabel::Str)::Cmk.Figure
    fig::Cmk.Figure = Cmk.Figure()
    ax::Cmk.Axis = Cmk.Axis(fig[1, 1], title=title, xlabel=xlabel, ylabel=ylabel)
    ys1::Vec{Flt} = getproperty(pmf1, Symbol(fieldName))
    ys2::Vec{Flt} = getproperty(pmf2, Symbol(fieldName))
    Cmk.lines!(ax, pmf1.names, ys1, label=label1)
    Cmk.lines!(ax, pmf2.names, ys2, label=label2)
    Cmk.axislegend();
    return fig
end

#+END_SRC

** other fns

#+BEGIN_SRC

function rep(x::A, times::Int)::Vec{A} where A
    @assert times > 1 "times must be greater than 1"
    return [x for _ in 1:times]
end

function rep(v::Vec{A}, times::Vec{Int})::Vec{A} where A
    @assert (len(v) == len(times)) "length(v) not equal length(times)"
    @assert all(map((>)(0), times)) "times elts must be GT 0"
    result::Vec{A} = Vec{eltype(v)}(undef, sum(times))
    currInd::Int = 1
    for i in eachindex(v)
        for _ in 1:times[i]
            result[currInd] = v[i]
            currInd += 1
        end
    end
    return result
end

c(sth) = collect(sth)
str(sth) = string(sth)

#+END_SRC

* 5.1. The Train Problem

The problem from Frederick Mosteller’s [[https://store.doverpublications.com/products/9780486653556][Fifty Challenging Problems in Probability with Solutions]]:

“A railroad numbers its locomotives in order 1…N. One day you see a locomotive with the number 60. Estimate how many locomotives the railroad has.”

Pmf:

#+BEGIN_SRC

train = getPmfFromSeq(1:1000 |> c)

#+END_SRC

Update:

#+BEGIN_SRC

updatePosteriors!(train, 60)

#+END_SRC

Posteriors:

#+BEGIN_SRC

drawPosteriors(train,
               "Posterior distribution\nafter locomative no. 60 was observed",
               "Number of trains", "PMF")

#+END_SRC

Max Posterior:

#+BEGIN_SRC

maxPostInd = getIndMaxPosterior(train)
train.names[maxPostInd], train.posteriors[maxPostInd]

#+END_SRC

*maxPostInd - maximizes the chance of getting the answer exactly right.*

What is the mean posterior destribution, i.e.

$mean = \sum_i (p_i * q_i)$

where:

- $q_i$ - a set of possible quantities
- $p_i$ - a set of possible probabilities for the quantities

#+BEGIN_SRC

sum(train.names .* train.posteriors)

#+END_SRC

Of course we can close it to a function (see PMF section) and use it like so:

#+BEGIN_SRC

getMeanPosteriorName(train)

#+END_SRC

*getMeanPosteriorName - minimizes the mean squared error* over a long run.

* 5.2. Sensitivity to the Prior

With small data set of observations the posterior is sensitive to the prior.

#+BEGIN_SRC

upBonds = [500, 1000, 2000]
posteriorMeans = Flt[]

for upBond in upBonds
    train = getPmfFromSeq(1:upBond |> c)
    updatePosteriors!(train, 60)
    push!(posteriorMeans, round3(getMeanPosteriorName(train)))
end

Dfs.DataFrame("upper bond" => upBonds, "posterior mean" => posteriorMeans)

#+END_SRC

With more data, the posteriors tend to converge, e.g. let's say we observed
trains: 60, 30, and 90.

#+BEGIN_SRC

upBonds = [500, 1000, 2000]
posteriorMeans = Flt[]
trainNums = [60, 30, 90]

for upBond in upBonds
    train = getPmfFromSeq(1:upBond |> c)
    updatePosteriors!(train, trainNums)
    push!(posteriorMeans, round3(getMeanPosteriorName(train)))
end

Dfs.DataFrame("upper bond" => upBonds, "posterior mean" => posteriorMeans)

#+END_SRC

Compare the above with section 4.4. Triangle Prior.

* 5.3. Power Law Prior

If more data are not available we may try to improve prior by gathering more
background information.

In fact, the distribution of company sizes tends to follow a power law, as
Robert Axtell reports in Science
(http://www.sciencemag.org/content/293/5536/1818.full.pdf).

Mathematically this can be expressed as:

$N \propto (\frac{1}{N})^{\alpha}$

where:

- $\alpha$ is a parameter that is often near 1

Since from mathematics we know that:

$a^{-n} = \frac{1}{a^{n}}$

So we can rewrite the above as:

$N \propto N^{-\alpha}$

Once we know that we may compare the impact of both priors (uniform vs. power
law) on the posteriors.

#+BEGIN_SRC

alpha = 1.0
nLocomotives = 1:1000 |> c
trainPowerLawPriors = Pmf(nLocomotives, nLocomotives.^(-alpha))
trainUniformPriors = getPmfFromSeq(nLocomotives)

#+END_SRC

First, let's see the priors:

#+BEGIN_SRC

drawProbsComparison(trainPowerLawPriors, trainUniformPriors, "priors",
                    "power law", "uniform",
                    "Prior distributions", "Number of trains", "PMF")

#+END_SRC

Now, the update:

#+BEGIN_SRC

dataset = [60]
updatePosteriors!(trainPowerLawPriors, dataset)
updatePosteriors!(trainUniformPriors, dataset)

#+END_SRC

And posteriors:

#+BEGIN_SRC

drawProbsComparison(trainPowerLawPriors, trainUniformPriors, "posteriors",
                    "power law", "uniform",
                    "Prior distributions", "Number of trains", "PMF")

#+END_SRC

Now, let's see how the better priors translate into posterior means.
First, a small dataset:

#+BEGIN_SRC

upBonds = [500, 1000, 2000]
alpha = 1.0
posteriorMeans = Flt[]
# small data set
trainNums = [60]

for upBond in upBonds
    nTrains = 1:upBond |> c
    train = Pmf(nTrains, nTrains.^(-alpha))
    updatePosteriors!(train, trainNums)
    push!(posteriorMeans, round3(getMeanPosteriorName(train)))
end

Dfs.DataFrame("upper bond" => upBonds, "posterior mean" => posteriorMeans)

#+END_SRC

Next, a larger dataset:

#+BEGIN_SRC

upBonds = [500, 1000, 2000]
alpha = 1.0
posteriorMeans = Flt[]
# larger data set 60
trainNums = [30, 60, 90]

for upBond in upBonds
    nTrains = 1:upBond |> c
    train = Pmf(nTrains, nTrains.^(-alpha))
    updatePosteriors!(train, trainNums)
    push!(posteriorMeans, round3(getMeanPosteriorName(train)))
end

Dfs.DataFrame("upper bond" => upBonds, "posterior mean" => posteriorMeans)

#+END_SRC

Compare with Section 5.2. Sensitivity to the Prior. The spread of posterior
means is (much) smaller.

* 5.4. Credible Intervals

Other way to summarize a posterior distribution (except of point estimates we
met so far) is with percentiles.

For instance (see the PMF section above) we may calculate the posterior probability that a company has less than or equal to 100 trains:

#+BEGIN_SRC

alpha = 1.0
nLocomotives = 1:1000 |> c
trainPowerLawPriors = Pmf(nLocomotives, nLocomotives.^(-alpha))
dataset = [60, 30, 90]
updatePosteriors!(trainPowerLawPriors, dataset)

#+END_SRC

and calculations:

#+BEGIN_SRC

getPosteriorLE(trainPowerLawPriors, 100)

#+END_SRC

We can also calculate the quantile (see the PMF section above):

#+BEGIN_SRC

getPosteriorQuantile(trainPowerLawPriors, 0.5)

#+END_SRC

or a 90% confidence interval for the number of trains:

#+BEGIN_SRC

getPosteriorQuantile.(Ref(trainPowerLawPriors), [0.05, 0.95])
getPosteriorCredibleInterval(trainPowerLawPriors, 0.9)

#+END_SRC

* 5.5. The German Tank Problem

Reasoning similar to the one presented in this chapter was used to solve [[https://en.wikipedia.org/wiki/German_tank_problem][the
German Tank Problem]] during WW2.

* 5.6. Informative Priors

Among Bayesians there are two approaches to choosing prior distributions:

- informative (based on the background information about the problem),
- uninformative (as unrestricted as possible)

Both got priors and cons, for instance in the spectrum of
objectiveness-subjectiveness.

If you have a lot of data then the choice of the prior doesn't really matter.
However, with a limited amount of data you will likely to get different results
(compare the train case with uniform priors and power law priors).

My impression is that the author of the book inclines rather towards informative
priors.

* 5.7. Summary

This chapter focused on estimating counts (or the size of a population).

In the next chapter we will talk about the odds, as an alternative way to
express probabilities, and Bayes Rule as an alternative form of Bayes's Theorem.

